### 引入urllib.request库进行爬虫爬取数据
~~~python
# 引入urllib.request库下的urlopen函数
from urllib.request import urlopen

url="http://www.baidu.com"

# 发起请求获取响应
resp = urlopen(url)

# 读取响应返回的是 bytes 需要进行解码，否则中文会乱码
print(resp.read().decode("utf-8"))
~~~
---

# 使用requests库进行爬虫爬取数据
### pip安装requests
~~~shell

# pip 安装 requests 如果速度比较慢请使用 清华源
# 清华源参考：https://mirrors.tuna.tsinghua.edu.cn/help/pypi/
pip install requests

# 临时使用 清华源
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple requests

# 升级 pip 到最新的版本 (>=10.0.0) 后进行设置长期默认使用 清华源
python -m pip install --upgrade pip
#  pip 默认源的网络连接较差，临时使用清华源来升级 pip
python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade pip
# 设置长期默认使用 清华源
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
~~~
---

### 使用requests库
~~~python
# 引入requests库
import requests

# 发起get请求
url = "https://www.sogou.com/web"

query = {
    "query": "中秋"
}

requestHeaders = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
}

# headers名称参数传递headers，params名称参数传递表单参数
resp = requests.get(url, headers = requestHeaders, params = query)
print("响应状态码", resp.status_code)
print("响应转换为文本（也可以转换为其他的数据格式，例如json）", resp.text)

for k, v in resp.headers.items():
    print(k, v)

# 注意关闭连接
resp.close()



# 发起post请求
body = {"fromappid": 38, "frommoduletype": 1, "toreturncode": 1, "result": -1, "struserid": "",
        "serialtime": 1695290478, "toappid": 1, "tomoduletype": 100, "tointer": 1, "businesstype": "101",
        "cgi": "https://101.qq.com/", "errormsg": "您还没有登录"}

# headers名称参数传递请求body
resp = requests.post("https://ams.game.qq.com/log",
                     params = {"sCloudApiName": "atm"},
                     data = body,
                     headers = requestHeaders)
print("响应状态码", resp.status_code)
print("响应转换为json", resp.json())

# 注意关闭连接
resp.close()
~~~
---

### 使用bs4解析 html 抓取内容
~~~python
import requests

# 引入bs4 需要提前安装 pip install -i https://pypi.tuna.tsinghua.edu.cn/simple bs4
from bs4 import BeautifulSoup

headers = {
    "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
}

resp = requests.get("https://v.douyu.com/", headers=headers)

# bs4使用html解析器响应文本
page = BeautifulSoup(resp.text, "html.parser")

# 通过find_all() 函数获取匹配全部的标签，匹配方式是标签名称和标签属性
# 或者find() 函数获取匹配的第一个标签，匹配方式是标签名称和标签属性
table = page.find_all("strong", attrs={"class": "Home-CategroyLabel"})

for i in table:
    print(i, i.text)

~~~
---

# 通过session会话自动携带cookie发起请求
~~~python
import requests

# 获取会话，通过会话发送的请求会自动带上cookie
session = requests.session()
body = {
    "username": "admin",
    "password": "admin",
}

resp = session.post(
    "https://xxx/login", params=body)

cookies = resp.cookies
print(resp.json())

# requests发起请求手动携带cookie
# userResp = requests.get("https://xxxx/userInfo",headers = {"Cookie": cookies})

# session会话发送的请求会自动带上cookie
userResp = session.get("https://xxxx/userInfo")

print(userResp.json())
~~~
---


### 通过代理发起请求
~~~python
import requests

# 免费代理服务器查找：https://www.zdaye.com/dayProxy.html
# 注意代理服务器使用的协议
resp = requests.get("https://www.bilibili.com/", proxies={"http": "http://:123.60.170.8:80"})

print(resp.text)
~~~
---
# sklearn的转换器
### 在特征工程中大部分的操作都是先创建一个转换器，然后由转换器调用fit_transform()函数处理数据
### fit_transform()函数分为 fit 和 transform 两个步骤
### fit 是进行计算数据
### transform 将fit 计算的结果带入公式进行数据转换

# 特征工程提取数据
1. 将任意数据（文本或图像）转换为可用于机器学习的数字特征，有字典特征提取（特征离散化）、文本特征提取、图像特征提取
2. 特征提取API sklearn.feature_extraction

### 字典特征提取
~~~python
# pip install scikit-learn
from sklearn.feature_extraction import DictVectorizer

data = [{"city": "上海", "temperature": 100},
        {"city": "北京", "temperature": 60},
        {"city": "深圳", "temperature": 3}]

# 创建转换器，DictVectorizer负责字典特征提取
# sparse参数表示是否使用稀疏矩阵
# 稀疏矩阵实际表示的是一个二维数组，记录了二维数组的坐标和对应坐标的值，但是没有记录的二维数组坐标上的值都是0
transfer = DictVectorizer(sparse=False)

# fit_transform()函数进行字典特征提取，返回一个矩阵，如果是稀疏矩阵可以继续调用 toarray() 将转换为正常的二维数组
# 即将字典中的全部value都提取成数字表示（除非原本就是数字），如果value相同则转换后的数字也相同
# 例如 上述例子转换后如下
# [[  1.   0.   0. 100.]
#  [  0.   1.   0.  60.]
#  [  0.   0.   1.   3.]]
# 其中每一列都表示一个value的值，并且使用one-hot编码，即0代表不是这个值，1代表是这个值
# 例如第1、2、3列分别表示上海、北京、深圳，而第四列的值之前就是数字无需转换
print(transfer.fit_transform(data))
~~~
---

### 文本特征提取
~~~python
# pip install scikit-learn
from sklearn.feature_extraction.text import CountVectorizer
# pip install jieba
import jieba

data = ["In the past, people have taken yellow crane to go, a yellow",
        "The yellow crane is gone forever",
        "Qingchuan calendar Hanyang tree",
        "Where is the twilight village pass? "]

# 创建转换器，CountVectorizer负责文本特征提取
# 提取单词出现的次数（不会提取标点符号和单字符，注意提取的是英文的单词，如果对中文需要有效果就需要使用空格分隔词语）
# stop_words参数 可以设置一些停用词，停用词是不会被进行提取的
transfer = CountVectorizer()

# toarray()将稀疏矩阵转为普通的二维数组
print("提取的特征数据\n", transfer.fit_transform(data).toarray())
print("特征名称", transfer.get_feature_names_out())


# 中文文本特征提取，需要提取对中文字符串进行分词，这里使用的是 jieba 库做的分词，但是一些古诗词还是没有办法做到完全分词
text = ["凤凰台上凤凰游，凤去台空江自流。",
        "吴宫花草埋幽径，晋代衣冠成古丘。",
        "三山半落青天外，二水中分白鹭洲。",
        "总为浮云能蔽日，长安不见使人愁"]
data = [" ".join(list(jieba.cut(t))) for t in text]

transfer = CountVectorizer()

# toarray()将稀疏矩阵转为普通的二维数组
print("提取的特征数据\n", transfer.fit_transform(data).toarray())
print("特征名称", transfer.get_feature_names_out())
~~~
---


## Tf-idf文本特征提取
Tf也叫词频，指一个词语在该文件中出现的次数
idf也叫逆向文本频率，由总文件数量除以包含指定词语出现的文件数目，再对得到的商取10的对数
由 Tf 与 idf 相乘的结果就是指定词语的重要程度
~~~python
# pip install scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer
# pip install jieba
import jieba

text = [
    """村里来了一个年轻的女人，大约有二十四五岁。她带着一个盛药的皮囊，到这里来行医看病。有的人去找她看病，她自己不能开药方子，要等到晚间问一问各位神仙。
晚上，她把一间小房子打扫得干干净净，把自己关在里面。大伙儿围绕在门窗口，斜着头侧着耳朵静静地听，只听里面在小声私语，谁也不敢咳嗽一声。
屋里屋外，黑洞洞的一片，没有一点动静。""",
    """大约到半夜的时候，忽然听到门帘微动的声音。女子在屋里说：“九姑来了吗？”一女子回答说：“来了。”
又问：“腊梅也跟着九姑来了？”好似一个丫头的声音，说：“来了。”三个人话语间杂，唠叨起来没个完。过了一会儿，又听到帘钩馓动的响声，女子说：“六姑来了？”
接着听到几个女子杂乱的说话声：“春梅也抱小郎君来了吗？”一个女子说：“这个顽皮的小家伙，怎么哄也不睡，定要跟来。身子有百十斤重，背着真累死人。
”马上又听到女子殷勤的接待声，九姑的问讯声，六姑与姊妹们的寒暄客套声，两个丫头的互相慰劳声，小孩儿的嘻闹声，一齐嘈嘈杂杂地传出来。
就听女子笑着说：“小郎君倒很好玩耍，老远的抱了个猫儿来。”接着说话的声音渐渐稀疏下来。门帘又响了一声，满屋里都喧哗起来，
说：“四姑来得怎么这样晚？”听到一个女孩子细微的声音，说：“路足有一千多里，我同阿姑走了这么长时间才到。阿姑走得太慢了。”
于是各人问寒问暖的声音，移动座位的声音，招呼着加座的声音，各种声音并作，喧闹满屋，有一顿饭的工夫才静下来。
接着就听到女子问病求药的声音。九姑说当用人参，六姑认为当用黄芪，四姑说该用白术。协商一会儿，听到九姑叫人拿笔墨砚台来。
不久，听到折纸的刷刷声，拔下笔帽扔到桌子上的丁丁声，隆隆的研墨声。接着就听到把笔投到桌几上的碰撞声，抓药包一皮纸的苏苏声。过了一会。
女子掀开门帘，招呼着病人的名字，把药包一皮和药方一起递了出来。她转身入室后，立刻听到三位姑娘作别的声音，三个丫头的道别声，小儿哑哑的叫声，小猫儿的呜呜声，
又一时并发起来。九姑的声音清晰悠扬，六姑的声音和缓苍老，四姑的声音娇滴宛转；以及三个丫头的声音，各有自己的特点，听着完全可以辨别得清楚。
大家感到很惊讶，认为真是神来了。回家试试药方，也并不灵验。这就是民间流传的口技，特意借这种方法卖药罢了。但她的口技水平，也真够高超的了。""",
    """以前，朋友王心逸曾讲过：他在京城时，偶尔从集市上经过，听到一阵管弦音乐的声音，围着看的人好像一堵墙。他到跟前一看，是一位少年，用优美的声音在演唱。
他手中并没有乐器，只用一个指头按着脸颊，一边按一边唱，听起来铿锵有声，与弦乐没什么差别。也是口技者的后代啊。"""]

data = [" ".join(list(jieba.cut(t))) for t in text]

# 创建转换器，TfidfVectorizer负责文本特征提取
# 提取单词的重要程度
# stop_words参数 可以设置一些停用词，停用词是不会被进行提取的
transfer = TfidfVectorizer()

# toarray()将稀疏矩阵转为普通的二维数组
print("提取词语Tf-idf特征数据\n", transfer.fit_transform(data).toarray())
print("特征名称", transfer.get_feature_names_out())
~~~
---

# 特征预处理（无量纲化）

## 特征数据归一化
### 计算公式 (x-min)*(mx-mi)/(max-min) + mi
### x 是当前的数据
### max 和 min 是列数据最大值和最小值
### mx 和 mi 是数据归一化结果的最大值和最小值，默认是1和0，即列中所有的数据进行处理后都在1和0的区间内

### 数据归一化的缺点是当列的最大值或最小值与其他数据差距较大时，归一化后的数据会进一步的拉大导致后续模型训练不稳定
~~~python
import io

# pip install scikit-learn
from sklearn.preprocessing import MinMaxScaler

# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
import pandas as pd

data_str = '''姓名,年龄,体重,身高
张三,25,51.2,165
李四,30,49.6,160
王五,35,55.8,169
张六,18,49.9,162
李七,26,60.3,170
王八,17,70.8,163
张九,19,66.2,172
李十,29,67.8,180
王一一,37,63.6,176
'''

# 这里是直接模拟的数据，也可以从csv文件读取数据
df = pd.read_csv(io.StringIO(data_str), encoding='utf-8').iloc[:,1:4]

print(df)

# feature_range参数设置 mx 和 mi ,即数据归一化结果的最大值和最小值
transfer = MinMaxScaler(feature_range=(2, 3))

print("数据归一化数据\n", transfer.fit_transform(df))
print("特征名称", transfer.get_feature_names_out())
~~~
---

## 特征数据标准化
### 计算公式 (x-mean)/std
### x 是当前的数据
### mean 是列数据的平均值
### std 是 标准差，标准差是一组数据平均值分散程度的一种度量。一个较大的标准差，代表大部分数值和其平均值之间差异较大；一个较小的标准差，代表这些数值较接近平均值。
### 标准化后的数据都在0附近，标准差是1左右

### 数据标准化的优点：由平均值和标准差都不会由于少量的数据与其他数据差距较大而产生大的变动，所以最终计算出的数据差距不会太大
~~~python
import io

# pip install scikit-learn
from sklearn.preprocessing import StandardScaler

# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
import pandas as pd

data_str = '''姓名,年龄,体重,身高
张三,25,51.2,165
李四,30,49.6,160
王五,35,55.8,169
张六,18,49.9,162
李七,26,60.3,170
王八,17,70.8,163
张九,19,66.2,172
李十,29,67.8,180
王一一,37,63.6,176
'''

# 这里是直接模拟的数据，也可以从csv文件读取数据
df = pd.read_csv(io.StringIO(data_str), encoding='utf-8').iloc[:,1:4]

print(df)

transfer = StandardScaler()

print("数据标准化数据\n", transfer.fit_transform(df))
print("特征名称", transfer.get_feature_names_out())
~~~
---

# 特征降维
### 降维是指去除有关联的特征数据
## 降维的方式：特征选择、主成分分析（可以理解为特征提取的一种方式）

### 特征选择
### 1. Filter(过滤式)
方差选择法：低方差特征过滤，即过滤掉特征样本数据大多数比较相近的特征
相关系数：计算系数根据系数过滤特征，不同的系数算法不同，如果特征之间的关联性比较强，有一些出来方式，例如会选择其中一个特征保留其他的过滤掉或者是进行加权求和、主动成分分析。如下有皮尔逊系数的计算使用API。


### 2. Embedded(嵌入式)
决策树：信息熵、信息增益
正则化：L1、L2
深度学习：卷积等
### 主成分分析（可以理解为特征提取的一种方式）
PCA算法进行数据降维

### 方差选择法使用
~~~python
import io

# pip install scikit-learn
from sklearn.feature_selection import VarianceThreshold

# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
import pandas as pd

data_str = '''姓名,年龄,体重,身高
张三,25,51.2,165
李四,23,49.6,160
王五,26,55.8,169
张六,24,49.9,162
李七,27,60.3,170
王八,26,70.8,163
张九,23,66.2,172
李十,24,67.8,180
王一一,26,63.6,176
'''

df = pd.read_csv(io.StringIO(data_str), encoding='utf-8').iloc[:,1:4]

print(df)

# 方差越大，数据的波动越大；方差越小，数据的波动就越小
# 过滤方差低于threshold参数的特征数据列
transfer = VarianceThreshold(threshold=2)

print("提取的特征数据\n", transfer.fit_transform(df))
print("特征名称", transfer.get_feature_names_out())
~~~
---


### 皮尔逊系数过滤特征
皮尔逊系数判断不同的特征其关联性如何

皮尔逊系数计算公式如下
$$ 
\frac {n\sum _{m=1} ^n (x_{m} y_{m}) - \sum _{m=1} ^n x_{m} \sum _{m=1} ^n y_{m}} \\\\
\sqrt[]{n \sum _{m=1} ^n x^2 - (\sum _{m=1} ^n x)^2} \sqrt[]{n \sum _{m=1} ^n y^2 - (\sum _{m=1} ^n y)^2}
$$

n表示数据的行数，x、y分别表示不同列即不同的特征

$n\sum _{m=1} ^n (x_{m} y_{m})$ 表示每一行的x特征值和y特征值相乘，然后将 n 行的结果相加再乘以 n  

$\sum _{m=1} ^n x_{m} \sum _{m=1} ^n y_{m} $ 表示x特征列和y特征列分别计算列中数据相加，然后将两个和相乘

$n \sum _{m=1} ^n x^2 $ 表示x特征值先开平方，然后相加得到和，再使用和乘以 n，另一段分母的计算也是一样

$(\sum _{m=1} ^n x)^2 $ 表示x特征值先求和，然后再开平方，另一段分母的计算也是一样

最终得到的皮尔逊系数是一个在-1到1之间的值，即 -1 <= r <= 1 ，系数的性质如下
1. 当r>0时表示是两个特征是正相关，r>0时两个特征是正相关。正相关表示其中一个数据增加另一个也增加，负相关表示其中一个数据增加另一个数据减少
2. 当 |r| =1 时表示两个特征完全相关，r=0时两个特征完全没有相关关系
3. 当 |r| 越接近 1 表示两个特征的相关关系越密切，r 越接近 0 表示两个特征的相关关系越弱
4. 系数一般分为3个级别，r < 0.4 为低度相关，0.4 <= r < 0.7 为显著相关，0.7 <= r < 1 为高度线性相关

~~~python
import io

# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
import pandas as pd

# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scipy
from scipy.stats import pearsonr


data_str = '''姓名,年龄,体重,身高
张三,25,51.2,165
李四,23,49.6,160
王五,26,55.8,169
张六,24,49.9,162
李七,27,60.3,170
王八,26,70.8,163
张九,23,66.2,172
李十,24,67.8,180
王一一,26,63.6,176
'''
df = pd.read_csv(io.StringIO(data_str), encoding='utf-8').iloc[:,1:4]

print(df)

print("年龄与身高的皮尔逊系数",pearsonr(df["年龄"],df["身高"]).statistic)
print("体重与身高的皮尔逊系数",pearsonr(df["体重"],df["身高"]).statistic)
~~~
---

# 主成分分析PCA算法使用
### PCA算法会在降维的过程中尽可能的保留数据的完整性，即尽可能的使用低维表示高维的数据
~~~python
# pip install scikit-learn
from sklearn.decomposition import PCA

data = [[1, 23, 56, 23], [23, 2, 6, 7], [3, 5, 7, 2]]

# n_components参数在1到原始数据的特征数之间的整数时会进行数据降维操作，降维到n_components参数设置的维数
# n_components参数在设置为0到1之间的小数则是保留百分比的数据完整性，但是不保证会按照百分比降维
transfer = PCA(n_components=2)
~~~
---
# sklearn的估计器
### 在机器学习算法中大部分的操作都是先创建一个估计器，然后由估计器调用 fit(x_train, y_train) 函数生成模型
### 在生成模型后进行模型评估，有两种方式：直接比对真实值和预测值、计算精确率
### 直接比对真实值和预测值：通过 predict(x_test) 函数计算测试数据的预估值
### 计算精确率：通过 score(x_test, y_test) 函数计算测试集精确率
---

# 过拟合和欠拟合
### 过拟合是模型学习的特征太多导致模型复杂度过高，在训练集的预测准确率高，但是在测试数据集或者其他的数据集上的预测准确低
### 欠拟合是模型学习的特征太少导致模型复杂度过低，在任何数据集中预测准确率都低
---

# K近邻算法（KNN）
### K近邻算法是：统计与目标数据节点最相似（最近）的 k 个节点的数据中大部分节点的数据是否属于某一类别，如果是则目标数据节点也是属于这个类别。用通俗的话就是物以类聚、近朱者赤近墨者黑
### K值过小会受到异常节点数据的影响，K值过大会受到样本不均衡的影响，通过特征工程的特征预处理的数据标准化可以有效的规避异常节点数据的影响
### K近邻算法的主要核心就是找到最相似（最近）的 k 个节点，根据不同的距离公式得到的 k 个节点不同
### 距离公式：欧式距离公式、曼哈顿距离、明可夫斯基距离等

### 欧式距离公式：其实也是求两点的空间中的距离公式，只是每个维度代表一个特征，如下是3维空间中两点的距离，即3个特征的数据节点之间的距离
$$ \sqrt[] {(a_1 - b_1)^2 + (a_2 - b_2)^2 + (a_3 - b_3)^2  } $$

### 曼哈顿距离：计算每种特征差的绝对值之和
$$ |a_1 - b_1| + |a_2 - b_2| + |a_3 - b_3| $$

### K近邻算法的缺点是K的值设置不当会严重影响计算的准确性，以及计算距离时计算量比较大比较占用内存只适用于小规模数据集计算

~~~python
# pip install scikit-learn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

# load_iris() 函数是获取鸢尾花的数据集，这是一个小规模的数据集
iris = load_iris()

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(iris["data"], iris["target"], test_size=0.25, random_state=31)

print("训练数据集的特征数据", x_train)
print("测试数据集的特征数据", x_test)
print("训练数据集的特征名称", y_train)
print("测试数据集的特征名称", y_test)

transfer = StandardScaler()
# 训练集数据标准化
x_train = transfer.fit_transform(x_train)

# 使用训练集计算的平均值和标准差来对测试集的数据进行标准化
x_test = transfer.transform(x_test)

# KNN算法预估器
# n_neighbors参数就是KNN算法的 K，通过K个最近节点计算，建议选择奇数，减少出现相同节点数量的结果
# algorithm参数是距离算法，有ball_tree、kd_tree、auto。默认是auto算法，auto算法会根据后续训练模型时传入的数据来决定合适的算法
estimator = KNeighborsClassifier(n_neighbors=3)
# 根据训练集获得模型
estimator.fit(x_train, y_train)

# 模型评估
# 方法一：直接比对真实值和预估值，即通过使用模型传入测试集的特征数据得到预估的特征名称
y_predict = estimator.predict(x_test)
print("真实值和预估值是否一致", y_test == y_predict)

# 方法二：计算精确率
score = estimator.score(x_test, y_test)
print("使用模型计算测试集的准确率", score)
~~~
---

# 交叉验证和预估器参数调优
为了得到模型的准确率，使用交叉验证
通过将训练集分为 n 份，每次拿出 1 份作为验证集，剩下的还是训练集，通过模型计算验证集的准确率。
重复上述步骤 n 次，得出一组准确率，计算平均值得出的准确率
根据n的数字也叫 n折交叉验证

### 交叉验证和预估器参数调优使用
~~~python
# pip install scikit-learn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# load_iris() 函数是获取鸢尾花的数据集，这是一个小规模的数据集
iris = load_iris()

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(iris["data"], iris["target"], test_size=0.25, random_state=31)

print("训练数据集的特征数据", x_train)
print("测试数据集的特征数据", x_test)
print("训练数据集的特征名称", y_train)
print("测试数据集的特征名称", y_test)

transfer = StandardScaler()
# 训练集数据标准化
x_train = transfer.fit_transform(x_train)

# 使用训练集计算的平均值和标准差来对测试集的数据进行标准化
x_test = transfer.transform(x_test)

param_dict = {"n_neighbors": [1, 3, 5, 7, 9, 11]}   
# GridSearchCV进行预估的参数调优和交叉验证，得出最佳的预估器参数和交叉验证的结果
# 第一个参数是预估器，如下是KNN算法预估器
# param_grid设置预估器的一组参数，当前 n_neighbors参数是KNN算法预估器的 K 值
# cv设置几折交叉验证
estimator = GridSearchCV(KNeighborsClassifier(), param_grid=param_dict, cv=10)

# 根据训练集获得模型
estimator.fit(x_train, y_train)

# 模型评估
# 方法一：直接比对真实值和预估值，即通过使用模型传入测试集的特征数据得到预估的特征名称
y_predict = estimator.predict(x_test)
print("真实值和预估值是否一致", y_test == y_predict)

# 方法二：计算精确率
score = estimator.score(x_test, y_test)
print("使用模型计算测试集的准确率", score)

print("最佳参数\n", estimator.best_params_)
print("验证集最佳结果\n", estimator.best_score_)
print("最佳预估器\n", estimator.best_estimator_)
print("交叉验证结果\n", estimator.cv_results_)
~~~
---

# 朴素贝叶斯分类算法

朴素贝叶斯分类算法是通过数据计算出类别出现的概率，其中朴素表示事件之间是相互独立的，贝叶斯表示贝叶斯公式

优点：对缺失数据不敏感（有拉普拉斯滑块系数），分类准确度较高，速度快
缺点：需要特征事件之间是相互独立的假设，当特征之间相互有关联时会影响分类结果

### 联合概率、条件概率与相互独立
联合概率：包含多个条件且全部条件同时成立的概率，P(A, B) 表示条件A和条件B同时成立
条件概率：事件A在事件B已经发生的条件下发生的概率，表示为P(A, B)
相互独立：P(A, B) = P(A)P(B) 则事件A和事件B相互独立

### 贝叶斯公式
$$ {P(C|W)} = \frac {P(W|C)P(C)} {P(W)} $$

### 注意当分子或者是分母为零时需要使用拉普拉斯平滑系数
一般情况下系数 a=1， m是事件C所在的特征有多少种事件
$$ {P(C|W)} = \frac {P(W|C)P(C) + a} {P(W) + am } $$

~~~python
# pip install scikit-learn
from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

# fetch_20newsgroups() 函数是获取新闻的数据集，这是一个大规模的数据集
# data_home参数设置大规模数据集下载到电脑的那个目录下
# subset参数设置是否训练数据集和测试数据集都要
news = fetch_20newsgroups(data_home="./data/",subset="all")

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(news["data"], news["target"], test_size=0.25, random_state=31)

# 文本特征抽取
transfer = TfidfVectorizer()

# 训练集数据文本特征抽取
x_train = transfer.fit_transform(x_train)

# 使用训练集的文本特征抽取方式来对测试集的数据进行文本特征抽取
x_test = transfer.transform(x_test)

# 朴素贝叶斯算法
# alpha参数设置拉普拉斯平滑系数，默认是1.0
estimator = MultinomialNB(alpha = 1.0)

# 根据训练集获得模型
estimator.fit(x_train, y_train)

# 模型评估
# 方法一：直接比对真实值和预估值，即通过使用模型传入测试集的特征数据得到预估的特征名称
y_predict = estimator.predict(x_test)
print("真实值和预估值是否一致", y_test == y_predict)

# 方法二：计算精确率
score = estimator.score(x_test, y_test)
print("使用模型计算测试集的准确率", score)
~~~
---

# 决策树
### 通过将特征按照重要性（重要性是指对结果的影响大小）编排成一棵决策树，根节点的特征重要性比叶子节点的特征重要性高
### 编排决策树的算法：信息增益（ID3）、信息增益比（C4.5）、CART

### 优点：决策树可视化
### 缺点：决策树过深，导致计算其他的数据集预测准确度不高，这被称为过拟合
### 改进：减枝cart算法和随机森林

~~~python
# pip install scikit-learn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier,export_graphviz
# load_iris() 函数是获取鸢尾花的数据集，这是一个小规模的数据集
iris = load_iris()

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(iris["data"], iris["target"], test_size=0.25, random_state=31)

# 决策树
# criterion参数设置决策树的算法，默认是gini，可以选择信息增益entropy
# max_depth参数设置决策树的最大深度，过于精细的决策树训练的模型可能这适用于当前模型，而对应其他数据集的结果预测准确率不高
# random_state参数设置随机树种子
estimator = DecisionTreeClassifier()

# 根据训练集获得模型
estimator.fit(x_train, y_train)

# 模型评估
# 方法一：直接比对真实值和预估值，即通过使用模型传入测试集的特征数据得到预估的特征名称
y_predict = estimator.predict(x_test)
print("真实值和预估值是否一致", y_test == y_predict)

# 方法二：计算精确率
score = estimator.score(x_test, y_test)
print("使用模型计算测试集的准确率", score)

# 可视化决策树
# 第一参数是模型
# out_file参数指定决策树输出到那个文件，注意文件后缀名是 .dot
# feature_names参数设置特征名称
# dot文件直接查看 http://webgraphviz.com/
# dot文件查看需要Graphviz安装及使用-决策树可视化 https://blog.csdn.net/NanamiKento/article/details/122748520
export_graphviz(estimator, out_file = "./tree/iris_tree.dot", feature_names = iris.feature_names)
~~~
---


# 随机森林
### 随机：训练集的样本随机选择 n 次（可能出现重复的样本），特征的数量也是随机选择出 m 个
### 森林：多棵决策树组成森林
### 通过多个决策树的进行预测结果在对结果投票，票多的就是最终的结果

~~~python
# pip install scikit-learn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# load_iris() 函数是获取鸢尾花的数据集，这是一个小规模的数据集
iris = load_iris()

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(iris["data"], iris["target"], test_size=0.25, random_state=31)

param_dict = {"n_estimators": [3, 5, 7],  # 森林的决策树数量调优
              # "criterion": ["gini", "gini", "gini"],  # 设置决策树的算法，默认是gini，可以选择信息增益entropy
              "max_depth": [1, 2, 3],  # 设置决策树的最大深度
              # "max_features": ["sqrt", "sqrt", "sqrt"],  # 设置最大的特征数，sqrt是开平方、log2是取对数
              # "bootstrap": [True, False, True],  # 随机选择训练集的样本训练后是否放回，下一次样本抽取重复的样本数据
              # "min_samples_split": [1, 2, 3],  # 节点划分最小样本树
              # "min_samples_leaf": [3, 2, 1]  # 叶子节点的最小样本数量
              }
# GridSearchCV进行预估的参数调优和交叉验证，得出最佳的预估器参数和交叉验证的结果
# 第一个参数是预估器，如下是 随机森林算法预估器
# param_grid设置预估器的一组参数
# cv设置几折交叉验证
estimator = GridSearchCV(RandomForestClassifier(), param_grid=param_dict, cv=3)

# 根据训练集获得模型
estimator.fit(x_train, y_train)

# 模型评估
# 方法一：直接比对真实值和预估值，即通过使用模型传入测试集的特征数据得到预估的特征名称
y_predict = estimator.predict(x_test)
print("真实值和预估值是否一致", y_test == y_predict)

# 方法二：计算精确率
score = estimator.score(x_test, y_test)
print("使用模型计算测试集的准确率", score)

print("最佳参数\n", estimator.best_params_)
print("验证集最佳结果\n", estimator.best_score_)
print("最佳预估器\n", estimator.best_estimator_)
print("交叉验证结果\n", estimator.cv_results_)
~~~
---



# 线性回归（正规方程和梯度下降）和 岭回归

### 一般情况下正规方差对比梯度下降的均方误差小，即更加准确，但是梯度下降可以通过调整算法参数来获取更加准确预测
### 正规方程不需要选择学习集，通过一次运算计算方程得出结果，时间复杂度是 $O（n^3）$ 适用于小规模数据集
### 梯度下降需要选择学习集，通过多次迭代运算得出结果，适用于大规模数据集
### 岭回归类似于梯度下降添加正则化L2参数，但是梯度下降添加正则化L2参数是随机梯度下降学习，而岭回归实现了SAG算法
~~~python
# pip install scikit-learn
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, SGDRegressor,Ridge
from sklearn.metrics import mean_squared_error
# fetch_california_housing() 函数是获取加州各街区的房屋数据集，目标是计算街区房屋的中位数
california_housing = fetch_california_housing(data_home='./data/')

print("特征数据\n", california_housing["data"])
print("特征名称\n", california_housing["feature_names"])
print("标签数据\n", california_housing["target"])
print("标签名称\n", california_housing["target_names"])
print("数据描述\n", california_housing["DESCR"])

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(california_housing["data"], california_housing["target"], test_size=0.25, random_state=31)

transfer = StandardScaler()
# 训练集数据标准化
x_train = transfer.fit_transform(x_train)

# 使用训练集计算的平均值和标准差来对测试集的数据进行标准化
x_test = transfer.transform(x_test)

# 线性回归之正规方程：通过参数可以调整算法准确性
estimator = LinearRegression()

# 线性回归之梯度下降：通过参数可以调整算法准确性
# estimator = SGDRegressor()

# 岭回归类似于梯度下降添加正则化L2参数，
# 但是梯度下降添加正则化L2参数是随机梯度下降学习，而岭回归实现了SAG
# estimator = Ridge()

# 根据训练集获得模型
estimator.fit(x_train, y_train)

print("回归系数为：", estimator.coef_)
print("截距为", estimator.intercept_)

# 使用均方误差进行模型评估，均方误差越小则模型越准确
y_predict = estimator.predict(x_test)
print("测试数据集使用模型预测街区房屋的中位数（当然需要使用无量纲化进一步还原数据）", y_predict)

error = mean_squared_error(y_test, y_predict)
print("均方误差为：", error)
~~~
---


# 逻辑回归进行数据预测分类

~~~python
# pip install scikit-learn
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report,roc_auc_score

# load_breast_cancer() 函数是获取乳腺癌威斯康星州(诊断)数据集，目标是计算街区房屋的中位数
breast_cancer = load_breast_cancer()

print("特征数据\n", breast_cancer["data"])
print("特征名称\n", breast_cancer["feature_names"])
print("标签数据\n", breast_cancer["target"])
print("标签名称\n", breast_cancer["target_names"])
print("数据描述\n", breast_cancer["DESCR"])

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(breast_cancer["data"],
                                                    breast_cancer["target"],
                                                    test_size=0.25, random_state=31)

transfer = StandardScaler()
# 训练集数据标准化
x_train = transfer.fit_transform(x_train)

# 使用训练集计算的平均值和标准差来对测试集的数据进行标准化
x_test = transfer.transform(x_test)

# 逻辑回归：随机梯度下降学习实现了SAG算法
estimator = LogisticRegression()

# 根据训练集获得模型
estimator.fit(x_train, y_train)

print("回归系数为：", estimator.coef_)
print("截距为", estimator.intercept_)

# 模型评估
# 方法一：直接比对真实值和预估值，即通过使用模型传入测试集的特征数据得到预估的特征名称
y_predict = estimator.predict(x_test)
print("测试数据集使用模型预测", y_predict)
print("真实值和预估值是否一致", y_test == y_predict)

# 方法二：计算精确率
score = estimator.score(x_test, y_test)
print("使用模型计算测试集的准确率", score)

# 计算精确率、召回率、F1-score
# 精确率是 预测正面效果正确数 比 预测的全部正面效果数
# 计算召回率，召回率是 预测正面效果正确数 比 实际的全部正面效果数
# F1-score是模型的稳健率
# labels参数和target_names参数设置分类结果的类别与对应名称
report = classification_report(y_test, y_predict, labels=[0, 1], target_names=["恶性", "良性"])
# 打印表格，precision、recall、f1-score、support分别是精确率、召回率、模型的稳健率、样本数量
print(report)


# 样本不均衡是需要计算AUC指标，注意真实值和预估值中的数据必须是0或者1
print(roc_auc_score(y_test, y_predict))
~~~
---


# Kmeans算法进行分类
算法流程：首先选出 n个 数据节点，计算其他节点到这 n个 数据节点的距离，按照最近的距离划分到对应节点的聚类中，这样就有 n个 聚类，然后计算 n个 聚类的中心点得到 新的n个数据节点，如果新的 n 个数据节点与原有的 n个 数据节点一致或者是相近则这n个数据节点就是最终分类的依据，会按照其他节点到这n个数据节点距离进行划分类别

### 算法缺陷：可能会出现 n个节点选取在相近的区域导致分类陷入局部最优解

~~~python
import io

# pip install scikit-learn
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas
import pandas as pd

data_str = '''姓名,年龄,体重,身高
张三,25,51.2,165
李四,30,49.6,160
王五,35,55.8,169
张六,18,49.9,162
李七,26,60.3,170
王八,17,70.8,163
张九,19,66.2,172
李十,29,67.8,180
王一一,37,63.6,176
'''

# 这里是直接模拟的数据，也可以从csv文件读取数据
df = pd.read_csv(io.StringIO(data_str), encoding='utf-8').iloc[:, 1:4]

print(df)

# Kmeans算法
# n_clusters 参数设置数据划分为多少种类别
# n_init 参数设置算法训练多少次，由于可能会受到初始n个节点位置的影响算法，通过多次的算法训练和每次初始都是随机选择n个节点，避免n个节点选取在相近的区域导致分类陷入局部最优解
estimator = KMeans(n_clusters=3, n_init=10)
# 根据数据集获得模型
estimator.fit(df)

y_predict = estimator.predict(df)
print("模型对数据集的分类结果", y_predict)

# Kmeans算法通过 轮廓系数 进行模型评估
# 轮廓系数取值在 -1 到 1之间范围，轮廓系数越接近 -1 则模型效果越差，越接近 1 则模型效果越好
# silhouette_score的第一参数是数据集，第二数据分类结果
print("Kmeans算法模型的轮廓系数是：", silhouette_score(df, y_predict))
~~~
---


# 模型的保存和加载

### 模型的保存，如下是鸢尾花数据集KNN算法的模型保存
~~~python
# pip install scikit-learn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
import joblib

# load_iris() 函数是获取鸢尾花的数据集，这是一个小规模的数据集
iris = load_iris()

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(iris["data"], iris["target"], test_size=0.25, random_state=31)

transfer = StandardScaler()
# 训练集数据标准化
x_train = transfer.fit_transform(x_train)

# 使用训练集计算的平均值和标准差来对测试集的数据进行标准化
x_test = transfer.transform(x_test)

# KNN算法预估器
# n_neighbors参数就是KNN算法的 K，通过K个最近节点计算，建议选择奇数，减少出现相同节点数量的结果
estimator = KNeighborsClassifier(n_neighbors=3)
# 根据训练集获得模型
estimator.fit(x_train, y_train)

# 模型评估
# 方法一：直接比对真实值和预估值，即通过使用模型传入测试集的特征数据得到预估的特征名称
y_predict = estimator.predict(x_test)
print("真实值和预估值是否一致", y_test == y_predict)

# 方法二：计算精确率
score = estimator.score(x_test, y_test)
print("使用模型计算测试集的准确率", score)

# 模型保存，注意模型必须保存为文件后缀名是 .pkl 的文件
joblib.dump(estimator,"./model/iris_Knn.pkl")
~~~
---

### 模型的加载
~~~python
# pip install scikit-learn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
import joblib

# load_iris() 函数是获取鸢尾花的数据集，这是一个小规模的数据集
iris = load_iris()

# train_test_split函数将数据集划分为训练数据集和测试数据集
# 返回值是一个元组，元素的表示如下
# 第一个元素是训练数据集的特征数据，第二个元素是测试数据集的特征数据
# 第三个元素是训练数据集的特征名称，第四个元素是测试数据集的特征名称
x_train, x_test, y_train, y_test = train_test_split(iris["data"], iris["target"], test_size=0.25, random_state=31)

transfer = StandardScaler()
# 训练集数据标准化
x_train = transfer.fit_transform(x_train)

# 使用训练集计算的平均值和标准差来对测试集的数据进行标准化
x_test = transfer.transform(x_test)

# 加载模型，通过pkl文件加载模型
estimator = joblib.load("./model/iris_Knn.pkl")

# 模型评估
# 方法一：直接比对真实值和预估值，即通过使用模型传入测试集的特征数据得到预估的特征名称
y_predict = estimator.predict(x_test)
print("真实值和预估值是否一致", y_test == y_predict)

# 方法二：计算精确率
score = estimator.score(x_test, y_test)
print("使用模型计算测试集的准确率", score)
~~~
---





